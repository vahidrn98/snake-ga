{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pygame\n",
    "import argparse\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from keras.utils import to_categorical\n",
    "from bayes_opt import BayesianOptimization,UtilityFunction\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,Flatten\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D\n",
    "import random\n",
    "import pandas as pd\n",
    "from operator import add\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, params):\n",
    "        self.reward = 0\n",
    "        self.gamma = 0.9\n",
    "        self.dataframe = pd.DataFrame()\n",
    "        self.short_memory = np.array([])\n",
    "        self.agent_target = 1\n",
    "        self.agent_predict = 0\n",
    "        self.learning_rate = params['learning_rate']        \n",
    "        self.epsilon = 1\n",
    "        self.actual = []\n",
    "        self.first_layer = params['first_layer_size']\n",
    "        self.second_layer = params['second_layer_size']\n",
    "        self.third_layer = params['third_layer_size']\n",
    "        self.memory = collections.deque(maxlen=params['memory_size'])\n",
    "        self.weights = params['weights_path']\n",
    "        self.load_weights = params['load_weights']\n",
    "        self.model = self.network()\n",
    "\n",
    "    def network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(28, (4, 4), activation='relu', input_shape=(28, 28, 1)))\n",
    "        model.add(MaxPooling2D((4, 4)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.first_layer, activation='tanh'))\n",
    "        model.add(Dense(self.second_layer, activation='tanh'))\n",
    "        model.add(Dense(self.third_layer, activation='tanh'))\n",
    "        # model.add(Dense(output_dim=self.third_layer, activation='tanh'))\n",
    "        # model.add(Dense(output_dim=self.third_layer, activation='tanh'))\n",
    "        model.add(Dense(3, activation='linear'))\n",
    "        opt = Adam(self.learning_rate)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        if self.load_weights:\n",
    "            model.load_weights(self.weights)\n",
    "        return model\n",
    "    \n",
    "    def get_state(self, game, player, food):\n",
    "        \n",
    "        initial = np.zeros(shape=(28,28,1))\n",
    "        for i in range(22):\n",
    "            if(i!=0 and i!=21):\n",
    "                initial[i+3][3] = [-10]\n",
    "                initial[i+3][24] = [-10]\n",
    "            else:\n",
    "                for j in range(22):\n",
    "                    if(i==0 or i==21):\n",
    "                        initial[i+3][j+3] = [-10]\n",
    "        # print(food.x_food)\n",
    "        initial[(food.x_food//20)+3][(food.y_food//20)+3] = [10]\n",
    "        for p in player.position:\n",
    "            initial[(int(p[0])//20)+3][(int(p[1])//20)+3] = [-10]\n",
    "        initial[(int(player.position[-1][0])//20)+3][(int(player.position[-1][1])//20)+3] =[ 1]\n",
    "        # for i in range(22):\n",
    "        #     print(initial[i])\n",
    "        state =initial\n",
    "        # state = [\n",
    "        #     (player.x_change == 20 and player.y_change == 0 and ((list(map(add, player.position[-1], [20, 0])) in player.position) or\n",
    "        #     player.position[-1][0] + 20 >= (game.game_width - 20))) or (player.x_change == -20 and player.y_change == 0 and ((list(map(add, player.position[-1], [-20, 0])) in player.position) or\n",
    "        #     player.position[-1][0] - 20 < 20)) or (player.x_change == 0 and player.y_change == -20 and ((list(map(add, player.position[-1], [0, -20])) in player.position) or\n",
    "        #     player.position[-1][-1] - 20 < 20)) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add, player.position[-1], [0, 20])) in player.position) or\n",
    "        #     player.position[-1][-1] + 20 >= (game.game_height-20))),  # danger straight\n",
    "\n",
    "        #     (player.x_change == 0 and player.y_change == -20 and ((list(map(add,player.position[-1],[20, 0])) in player.position) or\n",
    "        #     player.position[ -1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],\n",
    "        #     [-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == -20 and player.y_change == 0 and ((list(map(\n",
    "        #     add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n",
    "        #     (list(map(add,player.position[-1],[0,20])) in player.position) or player.position[-1][\n",
    "        #      -1] + 20 >= (game.game_height-20))),  # danger right\n",
    "\n",
    "        #      (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],[20,0])) in player.position) or\n",
    "        #      player.position[-1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == -20 and ((list(map(\n",
    "        #      add, player.position[-1],[-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n",
    "        #     (list(map(add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (\n",
    "        #     player.x_change == -20 and player.y_change == 0 and ((list(map(add,player.position[-1],[0,20])) in player.position) or\n",
    "        #     player.position[-1][-1] + 20 >= (game.game_height-20))), #danger left\n",
    "\n",
    "        #     player.x_change == -20,  # move left\n",
    "        #     player.x_change == 20,  # move right\n",
    "        #     player.y_change == -20,  # move up\n",
    "        #     player.y_change == 20,  # move down\n",
    "        #     food.x_food < player.x,  # food left\n",
    "        #     food.x_food > player.x,  # food right\n",
    "        #     food.y_food < player.y,  # food up\n",
    "        #     food.y_food > player.y  # food down\n",
    "        #     ]\n",
    "\n",
    "        # for i in range(len(state)):\n",
    "        #     if state[i]:\n",
    "        #         state[i]=1\n",
    "        #     else:\n",
    "        #         state[i]=0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def set_reward(self, player, crash):\n",
    "        self.reward = 0\n",
    "        if crash:\n",
    "            self.reward = -10\n",
    "            return self.reward\n",
    "        if player.eaten:\n",
    "            self.reward = 10\n",
    "        return self.reward\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay_new(self, memory, batch_size):\n",
    "        if len(memory) > batch_size:\n",
    "            minibatch = random.sample(memory, batch_size)\n",
    "        else:\n",
    "            minibatch = memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(np.array([next_state]))[0])\n",
    "            target_f = self.model.predict(np.array([state]))\n",
    "            target_f[0][np.argmax(action)] = target\n",
    "            self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target = reward + self.gamma * np.amax(self.model.predict(np.array([next_state]))[0])\n",
    "        target_f = self.model.predict(np.array([state]))\n",
    "        target_f[0][np.argmax(action)] = target\n",
    "        self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Conv2D (defined at c:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_281]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-58a285cfaef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;31m# for e in range(10):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-58a285cfaef8>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(display_option, speed, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;31m# Perform first move\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0minitialize_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfood1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdisplay_option\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfood1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-58a285cfaef8>\u001b[0m in \u001b[0;36minitialize_game\u001b[1;34m(player, game, food, agent, batch_size)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[0mreward1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrash\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_init1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_init2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrash\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-36574d759b6d>\u001b[0m in \u001b[0;36mreplay_new\u001b[1;34m(self, memory, batch_size)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mtarget_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m     return self._model_iteration(\n\u001b[0;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m               \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m               total_epochs=1)\n\u001b[0m\u001b[0;32m    445\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    524\u001b[0m               *args, **kwds)\n\u001b[0;32m    525\u001b[0m       \u001b[1;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mc:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Conv2D (defined at c:\\users\\padideh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_distributed_function_281]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "#################################\n",
    "#   Define parameters manually  #\n",
    "#################################\n",
    "def define_parameters(layer_size,mem,batch,alpha):\n",
    "    params = dict()\n",
    "    params['epsilon_decay_linear'] = 1/75\n",
    "    params['learning_rate'] = alpha\n",
    "    params['first_layer_size'] = layer_size  # neurons in the first layer\n",
    "    params['second_layer_size'] = layer_size   # neurons in the second layer\n",
    "    params['third_layer_size'] = layer_size   # neurons in the third layer\n",
    "    params['episodes'] = 150            \n",
    "    params['memory_size'] = mem\n",
    "    params['batch_size'] = batch\n",
    "    params['weights_path'] = 'weights/weights.hdf5'\n",
    "    params['load_weights'] = False\n",
    "    params['train'] = True\n",
    "    return params\n",
    "\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, game_width, game_height):\n",
    "        pygame.display.set_caption('SnakeGen')\n",
    "        self.game_width = game_width\n",
    "        self.game_height = game_height\n",
    "        self.gameDisplay = pygame.display.set_mode((game_width, game_height + 60))\n",
    "        self.bg = pygame.image.load(\"img/background.png\")\n",
    "        self.crash = False\n",
    "        self.player = Player(self)\n",
    "        self.food = Food()\n",
    "        self.score = 0\n",
    "\n",
    "\n",
    "class Player(object):\n",
    "    def __init__(self, game):\n",
    "        x = 0.45 * game.game_width\n",
    "        y = 0.5 * game.game_height\n",
    "        self.x = x - x % 20\n",
    "        self.y = y - y % 20\n",
    "        self.position = []\n",
    "        self.position.append([self.x, self.y])\n",
    "        self.food = 1\n",
    "        self.eaten = False\n",
    "        self.image = pygame.image.load('img/snakeBody.png')\n",
    "        self.x_change = 20\n",
    "        self.y_change = 0\n",
    "\n",
    "    def update_position(self, x, y):\n",
    "        if self.position[-1][0] != x or self.position[-1][1] != y:\n",
    "            if self.food > 1:\n",
    "                for i in range(0, self.food - 1):\n",
    "                    self.position[i][0], self.position[i][1] = self.position[i + 1]\n",
    "            self.position[-1][0] = x\n",
    "            self.position[-1][1] = y\n",
    "\n",
    "    def do_move(self, move, x, y, game, food, agent):\n",
    "        move_array = [self.x_change, self.y_change]\n",
    "\n",
    "        if self.eaten:\n",
    "            self.position.append([self.x, self.y])\n",
    "            self.eaten = False\n",
    "            self.food = self.food + 1\n",
    "        if np.array_equal(move, [1, 0, 0]):\n",
    "            move_array = self.x_change, self.y_change\n",
    "        elif np.array_equal(move, [0, 1, 0]) and self.y_change == 0:  # right - going horizontal\n",
    "            move_array = [0, self.x_change]\n",
    "        elif np.array_equal(move, [0, 1, 0]) and self.x_change == 0:  # right - going vertical\n",
    "            move_array = [-self.y_change, 0]\n",
    "        elif np.array_equal(move, [0, 0, 1]) and self.y_change == 0:  # left - going horizontal\n",
    "            move_array = [0, -self.x_change]\n",
    "        elif np.array_equal(move, [0, 0, 1]) and self.x_change == 0:  # left - going vertical\n",
    "            move_array = [self.y_change, 0]\n",
    "        self.x_change, self.y_change = move_array\n",
    "        self.x = x + self.x_change\n",
    "        self.y = y + self.y_change\n",
    "\n",
    "        if self.x < 20 or self.x > game.game_width - 40 \\\n",
    "                or self.y < 20 \\\n",
    "                or self.y > game.game_height - 40 \\\n",
    "                or [self.x, self.y] in self.position:\n",
    "            game.crash = True\n",
    "        eat(self, food, game)\n",
    "\n",
    "        self.update_position(self.x, self.y)\n",
    "\n",
    "    def display_player(self, x, y, food, game):\n",
    "        self.position[-1][0] = x\n",
    "        self.position[-1][1] = y\n",
    "\n",
    "        if game.crash == False:\n",
    "            for i in range(food):\n",
    "                x_temp, y_temp = self.position[len(self.position) - 1 - i]\n",
    "                game.gameDisplay.blit(self.image, (x_temp, y_temp))\n",
    "\n",
    "            update_screen()\n",
    "        else:\n",
    "            pygame.time.wait(300)\n",
    "\n",
    "\n",
    "class Food(object):\n",
    "    def __init__(self):\n",
    "        self.x_food = 240\n",
    "        self.y_food = 200\n",
    "        self.image = pygame.image.load('img/food2.png')\n",
    "\n",
    "    def food_coord(self, game, player):\n",
    "        x_rand = randint(20, game.game_width - 40)\n",
    "        self.x_food = x_rand - x_rand % 20\n",
    "        y_rand = randint(20, game.game_height - 40)\n",
    "        self.y_food = y_rand - y_rand % 20\n",
    "        if [self.x_food, self.y_food] not in player.position:\n",
    "            return self.x_food, self.y_food\n",
    "        else:\n",
    "            self.food_coord(game, player)\n",
    "\n",
    "    def display_food(self, x, y, game):\n",
    "        game.gameDisplay.blit(self.image, (x, y))\n",
    "        update_screen()\n",
    "\n",
    "\n",
    "def eat(player, food, game):\n",
    "    if player.x == food.x_food and player.y == food.y_food:\n",
    "        food.food_coord(game, player)\n",
    "        player.eaten = True\n",
    "        game.score = game.score + 1\n",
    "\n",
    "\n",
    "def get_record(score, record):\n",
    "    if score >= record:\n",
    "        return score\n",
    "    else:\n",
    "        return record\n",
    "\n",
    "\n",
    "def display_ui(game, score, record):\n",
    "    myfont = pygame.font.SysFont('Segoe UI', 20)\n",
    "    myfont_bold = pygame.font.SysFont('Segoe UI', 20, True)\n",
    "    text_score = myfont.render('SCORE: ', True, (0, 0, 0))\n",
    "    text_score_number = myfont.render(str(score), True, (0, 0, 0))\n",
    "    text_highest = myfont.render('HIGHEST SCORE: ', True, (0, 0, 0))\n",
    "    text_highest_number = myfont_bold.render(str(record), True, (0, 0, 0))\n",
    "    game.gameDisplay.blit(text_score, (45, 440))\n",
    "    game.gameDisplay.blit(text_score_number, (120, 440))\n",
    "    game.gameDisplay.blit(text_highest, (190, 440))\n",
    "    game.gameDisplay.blit(text_highest_number, (350, 440))\n",
    "    game.gameDisplay.blit(game.bg, (10, 10))\n",
    "\n",
    "\n",
    "def display(player, food, game, record):\n",
    "    game.gameDisplay.fill((255, 255, 255))\n",
    "    display_ui(game, game.score, record)\n",
    "    player.display_player(player.position[-1][0], player.position[-1][1], player.food, game)\n",
    "    food.display_food(food.x_food, food.y_food, game)\n",
    "\n",
    "\n",
    "def update_screen():\n",
    "    pygame.display.update()\n",
    "\n",
    "\n",
    "def initialize_game(player, game, food, agent, batch_size):\n",
    "    state_init1 = agent.get_state(game, player, food)  # [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
    "    action = [1, 0, 0]\n",
    "    player.do_move(action, player.x, player.y, game, food, agent)\n",
    "    state_init2 = agent.get_state(game, player, food)\n",
    "    reward1 = agent.set_reward(player, game.crash)\n",
    "    agent.remember(state_init1, action, reward1, state_init2, game.crash)\n",
    "    agent.replay_new(np.asarray(agent.memory), batch_size)\n",
    "\n",
    "\n",
    "def plot_seaborn(array_counter, array_score):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(\n",
    "        np.array([array_counter])[0],\n",
    "        np.array([array_score])[0],\n",
    "        color=\"b\",\n",
    "        x_jitter=.1,\n",
    "        line_kws={'color': 'green'}\n",
    "    )\n",
    "    ax.set(xlabel='games', ylabel='score')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run(display_option, speed, params):\n",
    "    pygame.init()\n",
    "    agent = DQNAgent(params)\n",
    "    weights_filepath = params['weights_path']\n",
    "    if params['load_weights']:\n",
    "        agent.model.load_weights(weights_filepath)\n",
    "        print(\"weights loaded\")\n",
    "\n",
    "    counter_games = 0\n",
    "    score_plot = []\n",
    "    counter_plot = []\n",
    "    record = 0\n",
    "    while counter_games < params['episodes']:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "        # Initialize classes\n",
    "        game = Game(440, 440)\n",
    "        player1 = game.player\n",
    "        food1 = game.food\n",
    "\n",
    "        # Perform first move\n",
    "        initialize_game(player1, game, food1, agent, params['batch_size'])\n",
    "        if display_option:\n",
    "            display(player1, food1, game, record)\n",
    "\n",
    "        while not game.crash:\n",
    "            if not params['train']:\n",
    "                agent.epsilon = 0\n",
    "            else:\n",
    "                # agent.epsilon is set to give randomness to actions\n",
    "                agent.epsilon = 1 - (counter_games * params['epsilon_decay_linear'])\n",
    "            # if(agent.epsilon<0.05):\n",
    "            #     agent.epsilon = 0.05\n",
    "            # get old state\n",
    "            state_old = agent.get_state(game, player1, food1)\n",
    "            x1 = player1.x_change\n",
    "            y1 = player1.y_change\n",
    "            # perform random actions based on agent.epsilon, or choose the action\n",
    "            if randint(0, 1) < agent.epsilon:\n",
    "                final_move = to_categorical(randint(0, 2), num_classes=3)\n",
    "            else:\n",
    "                # predict action based on the old state\n",
    "                prediction = agent.model.predict(np.array([state_old]))\n",
    "                final_move = to_categorical(np.argmax(prediction[0]), num_classes=3)\n",
    "\n",
    "            # perform new move and get new state\n",
    "            player1.do_move(final_move, player1.x, player1.y, game, food1, agent)\n",
    "            state_new = agent.get_state(game, player1, food1)\n",
    "\n",
    "            # set reward for the new state\n",
    "            reward = agent.set_reward(player1, game.crash)\n",
    "\n",
    "            if params['train']:\n",
    "                # train short memory base on the new action and state\n",
    "                agent.train_short_memory(state_old, final_move, reward, state_new, game.crash)\n",
    "                # store the new data into a long term memory\n",
    "                agent.remember(state_old, final_move, reward, state_new, game.crash)\n",
    "\n",
    "            record = get_record(game.score, record)\n",
    "            if display_option:\n",
    "                display(player1, food1, game, record)\n",
    "                pygame.time.wait(speed)\n",
    "        if params['train']:\n",
    "            agent.replay_new(np.array(agent.memory), params['batch_size'])\n",
    "        counter_games += 1\n",
    "        print(f'Game {counter_games}      Score: {game.score}')\n",
    "        score_plot.append(game.score)\n",
    "        counter_plot.append(counter_games)\n",
    "    if params['train']:\n",
    "        agent.model.save_weights(params['weights_path'])\n",
    "    # plot_seaborn(counter_plot, score_plot)\n",
    "    return (sum(score_plot)/len(score_plot))\n",
    "\n",
    "b_optimizer = BayesianOptimization(\n",
    "    f=None,\n",
    "    pbounds={'alpha':(0.0005,0.1),'batch_size': (50, 500),'layer_size':(30,100),'memory_size':(1000,5000)},\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")\n",
    "pygame.font.init()\n",
    "# for e in range(10):\n",
    "params = define_parameters(150,2500,500,0.0005)\n",
    "run(False, 10, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
