{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pygame","execution_count":4,"outputs":[{"output_type":"stream","text":"Collecting pygame\n  Downloading pygame-1.9.6-cp37-cp37m-manylinux1_x86_64.whl (11.4 MB)\n\u001b[K     |████████████████████████████████| 11.4 MB 13.3 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pygame\nSuccessfully installed pygame-1.9.6\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pygame\nimport argparse\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom random import randint\nfrom keras.utils import to_categorical\nfrom bayes_opt import BayesianOptimization,UtilityFunction\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout,Flatten\nfrom tensorflow.keras.layers import Conv2D,MaxPooling2D\nimport tensorflow as tf\nimport random\nimport pandas as pd\nfrom operator import add\nimport collections","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","execution_count":15,"outputs":[{"output_type":"stream","text":"Found GPU at: /device:GPU:0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DQNAgent(object):\n    def __init__(self, params):\n        self.reward = 0\n        self.gamma = 0.9\n        self.dataframe = pd.DataFrame()\n        self.short_memory = np.array([])\n        self.agent_target = 1\n        self.agent_predict = 0\n        self.learning_rate = params['learning_rate']        \n        self.epsilon = 1\n        self.actual = []\n        self.first_layer = params['first_layer_size']\n        self.second_layer = params['second_layer_size']\n        self.third_layer = params['third_layer_size']\n        self.memory = collections.deque(maxlen=params['memory_size'])\n        self.weights = params['weights_path']\n        self.load_weights = params['load_weights']\n        self.model = self.network()\n\n    def network(self):\n        model = Sequential()\n        model.add(Conv2D(28, (4, 4), activation='relu', input_shape=(28, 28, 1)))\n        model.add(MaxPooling2D((4, 4)))\n        model.add(Flatten())\n        model.add(Dense(self.first_layer, activation='tanh'))\n        model.add(Dense(self.second_layer, activation='tanh'))\n        model.add(Dense(self.third_layer, activation='tanh'))\n        # model.add(Dense(output_dim=self.third_layer, activation='tanh'))\n        # model.add(Dense(output_dim=self.third_layer, activation='tanh'))\n        model.add(Dense(3, activation='linear'))\n        opt = Adam(self.learning_rate)\n        model.compile(loss='mse', optimizer=opt)\n\n        if self.load_weights:\n            model.load_weights(self.weights)\n        return model\n    \n    def get_state(self, game, player, food):\n        \n        initial = np.zeros(shape=(28,28,1))\n        for i in range(22):\n            if(i!=0 and i!=21):\n                initial[i+3][3] = [-10]\n                initial[i+3][24] = [-10]\n            else:\n                for j in range(22):\n                    if(i==0 or i==21):\n                        initial[i+3][j+3] = [-10]\n        # print(food.x_food)\n        initial[(food.x_food//20)+3][(food.y_food//20)+3] = [10]\n        for p in player.position:\n            initial[(int(p[0])//20)+3][(int(p[1])//20)+3] = [-10]\n        initial[(int(player.position[-1][0])//20)+3][(int(player.position[-1][1])//20)+3] =[ 1]\n        # for i in range(22):\n        #     print(initial[i])\n        state =initial\n        # state = [\n        #     (player.x_change == 20 and player.y_change == 0 and ((list(map(add, player.position[-1], [20, 0])) in player.position) or\n        #     player.position[-1][0] + 20 >= (game.game_width - 20))) or (player.x_change == -20 and player.y_change == 0 and ((list(map(add, player.position[-1], [-20, 0])) in player.position) or\n        #     player.position[-1][0] - 20 < 20)) or (player.x_change == 0 and player.y_change == -20 and ((list(map(add, player.position[-1], [0, -20])) in player.position) or\n        #     player.position[-1][-1] - 20 < 20)) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add, player.position[-1], [0, 20])) in player.position) or\n        #     player.position[-1][-1] + 20 >= (game.game_height-20))),  # danger straight\n\n        #     (player.x_change == 0 and player.y_change == -20 and ((list(map(add,player.position[-1],[20, 0])) in player.position) or\n        #     player.position[ -1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],\n        #     [-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == -20 and player.y_change == 0 and ((list(map(\n        #     add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n        #     (list(map(add,player.position[-1],[0,20])) in player.position) or player.position[-1][\n        #      -1] + 20 >= (game.game_height-20))),  # danger right\n\n        #      (player.x_change == 0 and player.y_change == 20 and ((list(map(add,player.position[-1],[20,0])) in player.position) or\n        #      player.position[-1][0] + 20 > (game.game_width-20))) or (player.x_change == 0 and player.y_change == -20 and ((list(map(\n        #      add, player.position[-1],[-20,0])) in player.position) or player.position[-1][0] - 20 < 20)) or (player.x_change == 20 and player.y_change == 0 and (\n        #     (list(map(add,player.position[-1],[0,-20])) in player.position) or player.position[-1][-1] - 20 < 20)) or (\n        #     player.x_change == -20 and player.y_change == 0 and ((list(map(add,player.position[-1],[0,20])) in player.position) or\n        #     player.position[-1][-1] + 20 >= (game.game_height-20))), #danger left\n\n        #     player.x_change == -20,  # move left\n        #     player.x_change == 20,  # move right\n        #     player.y_change == -20,  # move up\n        #     player.y_change == 20,  # move down\n        #     food.x_food < player.x,  # food left\n        #     food.x_food > player.x,  # food right\n        #     food.y_food < player.y,  # food up\n        #     food.y_food > player.y  # food down\n        #     ]\n\n        # for i in range(len(state)):\n        #     if state[i]:\n        #         state[i]=1\n        #     else:\n        #         state[i]=0\n\n        return state\n\n    def set_reward(self, player, crash):\n        self.reward = 0\n        if crash:\n            self.reward = -10\n            return self.reward\n        if player.eaten:\n            self.reward = 10\n        return self.reward\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def replay_new(self, memory, batch_size):\n        if len(memory) > batch_size:\n            minibatch = random.sample(memory, batch_size)\n        else:\n            minibatch = memory\n        for state, action, reward, next_state, done in minibatch:\n            target = reward\n            if not done:\n                target = reward + self.gamma * np.amax(self.model.predict(np.array([next_state]))[0])\n            with tf.device('/gpu:0'):\n                target_f = self.model.predict(np.array([state]))\n                target_f[0][np.argmax(action)] = target\n                self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n\n    def train_short_memory(self, state, action, reward, next_state, done):\n        target = reward\n        if not done:\n            with tf.device('/gpu:0'):\n                target = reward + self.gamma * np.amax(self.model.predict(np.array([next_state]))[0])\n        with tf.device('/gpu:0'):\n            target_f = self.model.predict(np.array([state]))\n            target_f[0][np.argmax(action)] = target\n            self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n#################################\n#   Define parameters manually  #\n#################################\ndef define_parameters(layer_size,mem,batch,alpha):\n    params = dict()\n    params['epsilon_decay_linear'] = 1/75\n    params['learning_rate'] = alpha\n    params['first_layer_size'] = layer_size  # neurons in the first layer\n    params['second_layer_size'] = layer_size   # neurons in the second layer\n    params['third_layer_size'] = layer_size   # neurons in the third layer\n    params['episodes'] = 150            \n    params['memory_size'] = mem\n    params['batch_size'] = batch\n    params['weights_path'] = 'weights/weights.hdf5'\n    params['load_weights'] = False\n    params['train'] = True\n    return params\n\n\nclass Game:\n    def __init__(self, game_width, game_height):\n        self.game_width = game_width\n        self.game_height = game_height\n        self.crash = False\n        self.player = Player(self)\n        self.food = Food()\n        self.score = 0\n\n\nclass Player(object):\n    def __init__(self, game):\n        x = 0.45 * game.game_width\n        y = 0.5 * game.game_height\n        self.x = x - x % 20\n        self.y = y - y % 20\n        self.position = []\n        self.position.append([self.x, self.y])\n        self.food = 1\n        self.eaten = False\n        \n        self.x_change = 20\n        self.y_change = 0\n\n    def update_position(self, x, y):\n        if self.position[-1][0] != x or self.position[-1][1] != y:\n            if self.food > 1:\n                for i in range(0, self.food - 1):\n                    self.position[i][0], self.position[i][1] = self.position[i + 1]\n            self.position[-1][0] = x\n            self.position[-1][1] = y\n\n    def do_move(self, move, x, y, game, food, agent):\n        move_array = [self.x_change, self.y_change]\n\n        if self.eaten:\n            self.position.append([self.x, self.y])\n            self.eaten = False\n            self.food = self.food + 1\n        if np.array_equal(move, [1, 0, 0]):\n            move_array = self.x_change, self.y_change\n        elif np.array_equal(move, [0, 1, 0]) and self.y_change == 0:  # right - going horizontal\n            move_array = [0, self.x_change]\n        elif np.array_equal(move, [0, 1, 0]) and self.x_change == 0:  # right - going vertical\n            move_array = [-self.y_change, 0]\n        elif np.array_equal(move, [0, 0, 1]) and self.y_change == 0:  # left - going horizontal\n            move_array = [0, -self.x_change]\n        elif np.array_equal(move, [0, 0, 1]) and self.x_change == 0:  # left - going vertical\n            move_array = [self.y_change, 0]\n        self.x_change, self.y_change = move_array\n        self.x = x + self.x_change\n        self.y = y + self.y_change\n\n        if self.x < 20 or self.x > game.game_width - 40 \\\n                or self.y < 20 \\\n                or self.y > game.game_height - 40 \\\n                or [self.x, self.y] in self.position:\n            game.crash = True\n        eat(self, food, game)\n\n        self.update_position(self.x, self.y)\n\n    def display_player(self, x, y, food, game):\n        self.position[-1][0] = x\n        self.position[-1][1] = y\n\n        if game.crash == False:\n            for i in range(food):\n                x_temp, y_temp = self.position[len(self.position) - 1 - i]\n                \n        else:\n            pygame.time.wait(300)\n\n\nclass Food(object):\n    def __init__(self):\n        self.x_food = 240\n        self.y_food = 200\n       \n\n    def food_coord(self, game, player):\n        x_rand = randint(20, game.game_width - 40)\n        self.x_food = x_rand - x_rand % 20\n        y_rand = randint(20, game.game_height - 40)\n        self.y_food = y_rand - y_rand % 20\n        if [self.x_food, self.y_food] not in player.position:\n            return self.x_food, self.y_food\n        else:\n            self.food_coord(game, player)\n\n    def display_food(self, x, y, game):\n        game.gameDisplay.blit(self.image, (x, y))\n        update_screen()\n\n\ndef eat(player, food, game):\n    if player.x == food.x_food and player.y == food.y_food:\n        food.food_coord(game, player)\n        player.eaten = True\n        game.score = game.score + 1\n\n\ndef get_record(score, record):\n    if score >= record:\n        return score\n    else:\n        return record\n\n\ndef display_ui(game, score, record):\n    myfont = pygame.font.SysFont('Segoe UI', 20)\n    myfont_bold = pygame.font.SysFont('Segoe UI', 20, True)\n    text_score = myfont.render('SCORE: ', True, (0, 0, 0))\n    text_score_number = myfont.render(str(score), True, (0, 0, 0))\n    text_highest = myfont.render('HIGHEST SCORE: ', True, (0, 0, 0))\n    text_highest_number = myfont_bold.render(str(record), True, (0, 0, 0))\n\n\ndef display(player, food, game, record):\n    game.gameDisplay.fill((255, 255, 255))\n    display_ui(game, game.score, record)\n    player.display_player(player.position[-1][0], player.position[-1][1], player.food, game)\n    food.display_food(food.x_food, food.y_food, game)\n\n\ndef update_screen():\n    pygame.display.update()\n\n\ndef initialize_game(player, game, food, agent, batch_size):\n    state_init1 = agent.get_state(game, player, food)  # [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n    action = [1, 0, 0]\n    player.do_move(action, player.x, player.y, game, food, agent)\n    state_init2 = agent.get_state(game, player, food)\n    reward1 = agent.set_reward(player, game.crash)\n    agent.remember(state_init1, action, reward1, state_init2, game.crash)\n    agent.replay_new(np.asarray(agent.memory), batch_size)\n\n\ndef plot_seaborn(array_counter, array_score):\n    sns.set(color_codes=True)\n    ax = sns.regplot(\n        np.array([array_counter])[0],\n        np.array([array_score])[0],\n        color=\"b\",\n        x_jitter=.1,\n        line_kws={'color': 'green'}\n    )\n    ax.set(xlabel='games', ylabel='score')\n    plt.show()\n\n\ndef run(display_option, speed, params):\n    agent = DQNAgent(params)\n    weights_filepath = params['weights_path']\n    if params['load_weights']:\n        agent.model.load_weights(weights_filepath)\n        print(\"weights loaded\")\n\n    counter_games = 0\n    score_plot = []\n    counter_plot = []\n    record = 0\n    while counter_games < params['episodes']:\n        game = Game(440, 440)\n        player1 = game.player\n        food1 = game.food\n\n        # Perform first move\n        initialize_game(player1, game, food1, agent, params['batch_size'])\n\n        while not game.crash:\n            if not params['train']:\n                agent.epsilon = 0\n            else:\n                # agent.epsilon is set to give randomness to actions\n                agent.epsilon = 1 - (counter_games * params['epsilon_decay_linear'])\n            # if(agent.epsilon<0.05):\n            #     agent.epsilon = 0.05\n            # get old state\n            state_old = agent.get_state(game, player1, food1)\n            x1 = player1.x_change\n            y1 = player1.y_change\n            # perform random actions based on agent.epsilon, or choose the action\n            if randint(0, 1) < agent.epsilon:\n                final_move = to_categorical(randint(0, 2), num_classes=3)\n            else:\n                # predict action based on the old state\n                with tf.device('/gpu:0'):\n                    prediction = agent.model.predict(np.array([state_old]))\n                    final_move = to_categorical(np.argmax(prediction[0]), num_classes=3)\n\n            # perform new move and get new state\n            player1.do_move(final_move, player1.x, player1.y, game, food1, agent)\n            state_new = agent.get_state(game, player1, food1)\n\n            # set reward for the new state\n            reward = agent.set_reward(player1, game.crash)\n\n            if params['train']:\n                # train short memory base on the new action and state\n                agent.train_short_memory(state_old, final_move, reward, state_new, game.crash)\n                # store the new data into a long term memory\n                agent.remember(state_old, final_move, reward, state_new, game.crash)\n\n            record = get_record(game.score, record)\n               \n        if params['train']:\n            agent.replay_new(np.array(agent.memory), params['batch_size'])\n        counter_games += 1\n        print(f'Game {counter_games}      Score: {game.score}')\n        score_plot.append(game.score)\n        counter_plot.append(counter_games)\n    if params['train']:\n        agent.model.save_weights(params['weights_path'])\n    # plot_seaborn(counter_plot, score_plot)\n    return (sum(score_plot)/len(score_plot))\n\nb_optimizer = BayesianOptimization(\n    f=None,\n    pbounds={'alpha':(0.0005,0.1),'batch_size': (50, 500),'layer_size':(30,100),'memory_size':(1000,5000)},\n    verbose=2,\n    random_state=1,\n)\n# for e in range(10):\nparams = define_parameters(150,2500,500,0.0005)\nrun(False, 10, params)","execution_count":18,"outputs":[{"output_type":"stream","text":"Game 1      Score: 1\nGame 2      Score: 0\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-2f0d6c221cc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;31m# for e in range(10):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-2f0d6c221cc4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(display_option, speed, params)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# Perform first move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0minitialize_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfood1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-2f0d6c221cc4>\u001b[0m in \u001b[0;36minitialize_game\u001b[0;34m(player, game, food, agent, batch_size)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mreward1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_init1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_init2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-a06ae11c12e7>\u001b[0m in \u001b[0;36mreplay_new\u001b[0;34m(self, memory, batch_size)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_short_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1084\u001b[0m       data_handler._initial_epoch = (  # pylint: disable=protected-access\n\u001b[1;32m   1085\u001b[0m           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n\u001b[0;32m-> 1086\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[1;32m    694\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m    721\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         gen_experimental_dataset_ops.make_data_service_iterator(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3005\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   3006\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MakeIterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3007\u001b[0;31m         tld.op_callbacks, dataset, iterator)\n\u001b[0m\u001b[1;32m   3008\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3009\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}